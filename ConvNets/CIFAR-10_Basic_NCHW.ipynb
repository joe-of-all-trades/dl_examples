{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll build a simple convolutional neural network for CIFAR-10 image classification. Code contained in this project was based on Tensorflow 1.2.1 and python 3.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 6:\n",
      "Image - Min Value: 7 Max Value: 249\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fbcadd898>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 6\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    output = np.zeros([len(x), 10])\n",
    "    for idx, item in enumerate(x):\n",
    "        output[idx, item] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above function is equivalent to tf.one_hot(x, 10), but tensorflow module can not be pickled so we're sticking with the above implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "We will randomly shuffle the data, normalize them and save them in binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The above work is all saved so when we're revisiting this notebook we don't have to do those work again. We can start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import helper\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))\n",
    "valid_features = np.transpose(valid_features,(0, 3, 1, 2))\n",
    "batch_mean = np.mean(valid_features)\n",
    "batch_std = np.std(valid_features)\n",
    "valid_features = valid_features.astype(np.float32)\n",
    "for ii in range(valid_features.shape[0]):\n",
    "    valid_features[ii, :, :, :] = (valid_features[ii, :, :, :] - batch_mean) / batch_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, None, \"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x_tensor, conv_num_outputs, conv_ksize, conv_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    w = tf.get_variable(\"w\", shape=[conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[1], conv_num_outputs],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b = tf.Variable(tf.truncated_normal([conv_num_outputs], mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    wc = tf.nn.conv2d(x_tensor, w, strides=[1, 1, conv_strides[0], conv_strides[1]], padding='SAME', data_format=\"NCHW\")\n",
    "    z = tf.nn.bias_add(wc, b, data_format=\"NCHW\")\n",
    "    print(x_tensor.get_shape().as_list())\n",
    "    return tf.nn.relu(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten layer\n",
    "Implement the flatten function to change the dimension of x_tensor from a 4-D tensor to a 2-D tensor. The output should be the shape (Batch Size, Flattened Image Size). Shortcut option: you can use classes from the TensorFlow Layers or TensorFlow Layers (contrib) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # reference : https://github.com/tensorflow/tensorflow/issues/7253\n",
    "    return tf.reshape(x_tensor, [tf.shape(x_tensor)[0], np.prod(x_tensor.get_shape().as_list()[1:])])\n",
    "    \n",
    "    # This also works\n",
    "    #return tf.reshape(x_tensor, [-1, np.prod(x_tensor.shape[1:]).value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    fc = tf.reshape(x_tensor, [-1, np.prod(x_tensor.get_shape().as_list()[1:])])\n",
    "    \n",
    "    w = tf.get_variable(\"w\", shape=[np.prod(x_tensor.get_shape().as_list()[1:]), num_outputs],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b = tf.Variable(tf.truncated_normal([num_outputs],mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    z = tf.add(tf.matmul(fc, w), b)\n",
    "    \n",
    "    return tf.nn.relu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    w = tf.get_variable(\"w\", shape=[np.prod(x_tensor.get_shape().as_list()[1:]), num_outputs],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b = tf.Variable(tf.truncated_normal([num_outputs],mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    return tf.add(tf.matmul(x_tensor, w), b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the convolutional neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Convolution and maxpooling layers\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        conv1 = conv2d(x, 40, (3, 3), (1, 1))\n",
    "        \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        conv2 = conv2d(conv1, 80, (3, 3), (1, 1))\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 1, 2, 2], strides=[1, 1, 2, 2], padding='SAME', data_format=\"NCHW\")\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        conv3 = conv2d(conv2, 160, (3, 3), (1, 1))\n",
    "    \n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        conv4 = conv2d(conv3, 320, (3, 3), (1, 1))\n",
    "    conv4 = tf.nn.max_pool(conv4, ksize=[1, 1, 2, 2], strides=[1, 1, 2, 2], padding='SAME', data_format=\"NCHW\")\n",
    "    conv4 = tf.nn.dropout(conv4, keep_prob)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    f = flatten(conv4)\n",
    "\n",
    "    # Fully Connected layers\n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        fc1 = fully_conn(f, 512)\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        fc2 = fully_conn(fc1, 256)\n",
    "        fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # Output Layer\n",
    "    with tf.variable_scope(\"out\"):\n",
    "        o = output(fc2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 3, 32, 32]\n",
      "[None, 40, 32, 32]\n",
      "[None, 80, 16, 16]\n",
      "[None, 160, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "# Channel first data format has hugh performance advantage!\n",
    "# It's at least three times faster than channel last format.\n",
    "x = neural_net_image_input((3, 32, 32))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(epsilon=1e-04).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Stats\n",
    "It's important to evaluate the performance of model once in a while. If effect, we're feeding a small batch of data to the neural network through forward propagation and then caculate the accuracy of prediction. We don't want to do this too often as this slows down the overall process. It's important to keep in mind that since we're actually using the model for prediction but not training it, we need to set keep probability for dropout to 1 so we're not losing any connection between neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    \n",
    "    valid_acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a single CIFAR-10 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "training one batch took: 0.7834 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0303 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0309 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0295 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0323 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0243 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0243 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0779 seconds\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.0469 Validation Accuracy: 0.306800\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0243 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0244 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0308 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0296 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0318 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0110 seconds\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.3990 Validation Accuracy: 0.370200\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0304 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0288 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0293 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0303 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0274 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0298 seconds\n",
      "training one batch took: 0.0333 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0142 seconds\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.3420 Validation Accuracy: 0.458800\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0240 seconds\n",
      "training one batch took: 0.0239 seconds\n",
      "training one batch took: 0.0242 seconds\n",
      "training one batch took: 0.0241 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0379 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0295 seconds\n",
      "training one batch took: 0.0325 seconds\n",
      "training one batch took: 0.0308 seconds\n",
      "training one batch took: 0.0332 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0112 seconds\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.1709 Validation Accuracy: 0.507000\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0243 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0293 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0309 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0124 seconds\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.1119 Validation Accuracy: 0.520600\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0296 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0288 seconds\n",
      "training one batch took: 0.0316 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0296 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0308 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0342 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0309 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0306 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0113 seconds\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.0152 Validation Accuracy: 0.557800\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0244 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0293 seconds\n",
      "training one batch took: 0.0315 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0241 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0295 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0304 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0304 seconds\n",
      "training one batch took: 0.0326 seconds\n",
      "training one batch took: 0.0296 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0132 seconds\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.8613 Validation Accuracy: 0.591400\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0244 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0305 seconds\n",
      "training one batch took: 0.0121 seconds\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.0614 Validation Accuracy: 0.610400\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0249 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0306 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0301 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0138 seconds\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.6284 Validation Accuracy: 0.619600\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0125 seconds\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.6242 Validation Accuracy: 0.626600\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0293 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0306 seconds\n",
      "training one batch took: 0.0293 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0313 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0316 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0115 seconds\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.5375 Validation Accuracy: 0.647400\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0110 seconds\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.5359 Validation Accuracy: 0.648400\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0334 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0243 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0244 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0297 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0147 seconds\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.4205 Validation Accuracy: 0.663200\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0110 seconds\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.5997 Validation Accuracy: 0.675400\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0314 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0326 seconds\n",
      "training one batch took: 0.0133 seconds\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.3229 Validation Accuracy: 0.686600\n",
      "training one batch took: 0.0288 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0311 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0347 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0296 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0294 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0296 seconds\n",
      "training one batch took: 0.0341 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0314 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0307 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0114 seconds\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.5781 Validation Accuracy: 0.679200\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0322 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0297 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0293 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0112 seconds\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.1958 Validation Accuracy: 0.693800\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0249 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0117 seconds\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.2066 Validation Accuracy: 0.697200\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0306 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0247 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0294 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0115 seconds\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.2853 Validation Accuracy: 0.693000\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0295 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0116 seconds\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.0778 Validation Accuracy: 0.705000\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0114 seconds\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.2115 Validation Accuracy: 0.696400\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0242 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0298 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0288 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0114 seconds\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.1765 Validation Accuracy: 0.692800\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0301 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0297 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0115 seconds\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.1126 Validation Accuracy: 0.706600\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0286 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0114 seconds\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.0968 Validation Accuracy: 0.692600\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0239 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0288 seconds\n",
      "training one batch took: 0.0295 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0118 seconds\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.0623 Validation Accuracy: 0.705400\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0292 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0283 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0294 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0114 seconds\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.0796 Validation Accuracy: 0.697600\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0294 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0301 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0116 seconds\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.1419 Validation Accuracy: 0.697400\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0296 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0290 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0278 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0289 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0115 seconds\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.0485 Validation Accuracy: 0.710400\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0249 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0308 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0267 seconds\n",
      "training one batch took: 0.0326 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0261 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0274 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0258 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0277 seconds\n",
      "training one batch took: 0.0300 seconds\n",
      "training one batch took: 0.0295 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0294 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0284 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0264 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0270 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0256 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0282 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0280 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0117 seconds\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.0468 Validation Accuracy: 0.708000\n",
      "training one batch took: 0.0281 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0263 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0273 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0276 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0294 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0285 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0268 seconds\n",
      "training one batch took: 0.0272 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0275 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0293 seconds\n",
      "training one batch took: 0.0288 seconds\n",
      "training one batch took: 0.0326 seconds\n",
      "training one batch took: 0.0307 seconds\n",
      "training one batch took: 0.0324 seconds\n",
      "training one batch took: 0.0287 seconds\n",
      "training one batch took: 0.0299 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0265 seconds\n",
      "training one batch took: 0.0251 seconds\n",
      "training one batch took: 0.0246 seconds\n",
      "training one batch took: 0.0248 seconds\n",
      "training one batch took: 0.0262 seconds\n",
      "training one batch took: 0.0245 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0253 seconds\n",
      "training one batch took: 0.0258 seconds\n",
      "training one batch took: 0.0260 seconds\n",
      "training one batch took: 0.0271 seconds\n",
      "training one batch took: 0.0255 seconds\n",
      "training one batch took: 0.0269 seconds\n",
      "training one batch took: 0.0266 seconds\n",
      "training one batch took: 0.0279 seconds\n",
      "training one batch took: 0.0259 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0257 seconds\n",
      "training one batch took: 0.0291 seconds\n",
      "training one batch took: 0.0250 seconds\n",
      "training one batch took: 0.0252 seconds\n",
      "training one batch took: 0.0254 seconds\n",
      "training one batch took: 0.0122 seconds\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.0260 Validation Accuracy: 0.706400\n",
      "total took:86.6213 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            batch_mean = np.mean(batch_features)\n",
    "            batch_std = np.std(batch_features)\n",
    "            batch_features = batch_features.astype(np.float32)\n",
    "            for ii in range(batch_features.shape[0]):\n",
    "                batch_features[ii, :, :, :] = (batch_features[ii, :, :, :] - batch_mean) / batch_std \n",
    "            batch_features = np.transpose(batch_features, (0, 3, 1, 2))\n",
    "            start = time.time()\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            end = time.time()\n",
    "            print('training one batch took: {:0.4f} seconds'.format(end-start))\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "t2 = time.time()\n",
    "print('total took:{:0.4f} seconds'.format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            \n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                \n",
    "                batch_mean = np.mean(batch_features)\n",
    "                batch_std = np.std(batch_features)\n",
    "                batch_features = batch_features.astype(np.float32)\n",
    "                for ii in range(batch_features.shape[0]):\n",
    "                    batch_features[ii, :, :, :] = (batch_features[ii, :, :, :] - batch_mean) / batch_std \n",
    "                batch_features = np.transpose(batch_features, (0, 3, 1, 2))\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    test_features = test_features.astype(np.float32)\n",
    "    batch_mean = np.mean(test_features)\n",
    "    batch_std = np.std(test_features)\n",
    "    for ii in range(test_features.shape[0]):\n",
    "        test_features[ii,:,:,:] = (test_features[ii,:,:,:] - batch_mean) / batch_std\n",
    "    \n",
    "    loaded_graph = tf.Graph()\n",
    "    \n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_feature_batch = np.transpose(test_feature_batch, (0, 3, 1, 2))\n",
    "            test_batch_acc_total += sess.run(loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_features = np.transpose(random_test_features, (0, 3, 1, 2))\n",
    "        random_test_predictions = sess.run(tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        random_test_features = np.transpose(random_test_features, (0, 2, 3, 1))\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_1_2]",
   "language": "python",
   "name": "conda-env-tf_1_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
