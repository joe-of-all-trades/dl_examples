{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll build a simple convolutional neural network for CIFAR-10 image classification. Code contained in this project was based on Tensorflow 1.2.1 and python 3.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 6:\n",
      "Image - Min Value: 7 Max Value: 249\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fba14e11588>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 6\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    \"\"\"\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "    # return ( x - x.min() ) / ( x.max() - x.min() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    smooth_factor = 0.1\n",
    "    output = np.zeros([len(x), 10]) # +( 0.1 / 9 )\n",
    "    for idx, item in enumerate(x):\n",
    "        output[idx, item] = 1. # - smooth_factor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above function is equivalent to tf.one_hot(x, 10), but tensorflow module can not be pickled so we're sticking with the above implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "We will randomly shuffle the data, normalize them and save them in binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The above work is all saved so when we're revisiting this notebook we don't have to do those work again. We can start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import helper\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))\n",
    "valid_features = normalize(valid_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input(n):\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, None, \"keep_prob_\"+str(n))\n",
    "\n",
    "\n",
    "def neural_net_training_flag():\n",
    "    return tf.placeholder(tf.bool, None, \"train_flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x_tensor, conv_num_outputs, conv_ksize, conv_strides, is_train):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    w = tf.get_variable(\"w\", shape=[conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[3], conv_num_outputs],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    #b = tf.Variable(tf.truncated_normal([conv_num_outputs],\n",
    "    #                                  mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    wc = tf.nn.conv2d(x_tensor, w, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    # z = tf.nn.bias_add(wc, b)\n",
    "    z = tf.layers.batch_normalization(wc, training=is_train)\n",
    "    \n",
    "    return tf.nn.relu(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten layer\n",
    "Implement the flatten function to change the dimension of x_tensor from a 4-D tensor to a 2-D tensor. The output should be the shape (Batch Size, Flattened Image Size). Shortcut option: you can use classes from the TensorFlow Layers or TensorFlow Layers (contrib) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # reference : https://github.com/tensorflow/tensorflow/issues/7253\n",
    "    return tf.reshape(x_tensor, [tf.shape(x_tensor)[0], np.prod(x_tensor.get_shape().as_list()[1:])])\n",
    "    \n",
    "    # This also works\n",
    "    #return tf.reshape(x_tensor, [-1, np.prod(x_tensor.shape[1:]).value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs, is_train):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    fc = tf.reshape(x_tensor, [-1, np.prod(x_tensor.get_shape().as_list()[1:])])\n",
    "    \n",
    "    w = tf.get_variable(\"w\", shape=[np.prod(x_tensor.get_shape().as_list()[1:]), num_outputs],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    # b = tf.Variable(tf.truncated_normal([num_outputs],mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    z = tf.matmul(fc, w)\n",
    "    z = tf.layers.batch_normalization(z, training=is_train)\n",
    "    \n",
    "    return tf.nn.relu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    w = tf.get_variable(\"w\", shape=[np.prod(x_tensor.get_shape().as_list()[1:]), num_outputs],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b = tf.Variable(tf.truncated_normal([num_outputs],mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    return tf.nn.softmax(tf.add(tf.matmul(x_tensor, w), b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the convolutional neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob_1, keep_prob_2, keep_prob_3, keep_prob_4,  train_flag):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convolution and maxpooling layers\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        conv1 = conv2d(x, 64, (3, 3), (1, 1), train_flag)\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob_1)\n",
    "    with tf.variable_scope(\"conv1s\"):\n",
    "        conv1s = conv2d(conv1, 64, (3, 3), (1, 1), train_flag)\n",
    "    conv1s = tf.nn.max_pool(conv1s, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv1s = tf.nn.dropout(conv1s, keep_prob_1)\n",
    "    \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        conv2 = conv2d(conv1s, 128, (3, 3), (1, 1), train_flag)\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob_2)\n",
    "    with tf.variable_scope(\"conv2s\"):\n",
    "        conv2s = conv2d(conv2, 128, (3, 3), (1, 1), train_flag)\n",
    "    conv2s = tf.nn.max_pool(conv2s, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv2s = tf.nn.dropout(conv2s, keep_prob_2)\n",
    "    \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        conv3 = conv2d(conv2s, 256, (3, 3), (1, 1), train_flag)\n",
    "    conv3 = tf.nn.dropout(conv3, keep_prob_3)\n",
    "    with tf.variable_scope(\"conv3s\"):\n",
    "        conv3s = conv2d(conv3, 256, (3, 3), (1, 1), train_flag)\n",
    "    conv3s = tf.nn.dropout(conv3s, keep_prob_3)\n",
    "    with tf.variable_scope(\"conv3t\"):\n",
    "        conv3t = conv2d(conv3s, 256, (3, 3), (1, 1), train_flag)\n",
    "    conv3t = tf.nn.max_pool(conv3t, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv3t = tf.nn.dropout(conv3t, keep_prob_3)\n",
    "    \n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        conv4 = conv2d(conv3s, 512, (3, 3), (1, 1), train_flag)\n",
    "    conv4 = tf.nn.dropout(conv4, keep_prob_4)\n",
    "    with tf.variable_scope(\"conv4s\"):\n",
    "        conv4s = conv2d(conv4, 512, (3, 3), (1, 1), train_flag)\n",
    "    conv4s = tf.nn.dropout(conv4s, keep_prob_4)\n",
    "    with tf.variable_scope(\"conv4t\"):\n",
    "        conv4t = conv2d(conv4s, 512, (3, 3), (1, 1), train_flag)\n",
    "    conv4t = tf.nn.max_pool(conv4t, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv4t = tf.nn.dropout(conv4t, keep_prob_4)\n",
    "    \n",
    "    \n",
    "    # Flatten Layer\n",
    "    f = flatten(conv4t)\n",
    "\n",
    "    # Fully Connected layers\n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        fc1 = fully_conn(f, 512, train_flag)\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob_4)\n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        fc2 = fully_conn(fc1, 512, train_flag)\n",
    "        fc2 = tf.nn.dropout(fc2, keep_prob_4)\n",
    "    #with tf.variable_scope(\"fc3\"):\n",
    "    #    fc3 = fully_conn(fc2, 256, train_flag)\n",
    "    #    fc3 = tf.nn.dropout(fc3, keep_prob_4)\n",
    "    \n",
    "    # Output Layer\n",
    "    with tf.variable_scope(\"out\"):\n",
    "        o = output(fc2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob_1 = neural_net_keep_prob_input(1)\n",
    "keep_prob_2 = neural_net_keep_prob_input(2)\n",
    "keep_prob_3 = neural_net_keep_prob_input(3)\n",
    "keep_prob_4 = neural_net_keep_prob_input(4)\n",
    "train_flag = neural_net_training_flag()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob_1, keep_prob_2, keep_prob_3, keep_prob_4, train_flag)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# Collect batch mean and variance for batch normalization\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, kp1, kp2, kp3, kp4, feature_batch, label_batch, is_train):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, \n",
    "                                      y: label_batch, \n",
    "                                      keep_prob_1: kp1,\n",
    "                                      keep_prob_2: kp2,\n",
    "                                      keep_prob_3: kp3,\n",
    "                                      keep_prob_4: kp4,\n",
    "                                      train_flag:is_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Stats\n",
    "It's important to evaluate the performance of model once in a while. If effect, we're feeding a small batch of data to the neural network through forward propagation and then caculate the accuracy of prediction. We don't want to do this too often as this slows down the overall process. It's important to keep in mind that since we're actually using the model for prediction but not training it, we need to set keep probability for dropout to 1 so we're not losing any connection between neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, \n",
    "                                        y: label_batch, \n",
    "                                        keep_prob_1: 1., \n",
    "                                        keep_prob_2: 1.,\n",
    "                                        keep_prob_3: 1.,\n",
    "                                        keep_prob_4: 1.,\n",
    "                                        train_flag:False})\n",
    "    valid_acc = session.run(accuracy, feed_dict={x: valid_features, \n",
    "                                                 y: valid_labels, \n",
    "                                                 keep_prob_1: 1., \n",
    "                                                 keep_prob_2: 1.,\n",
    "                                                 keep_prob_3: 1.,\n",
    "                                                 keep_prob_4: 1.,\n",
    "                                                 train_flag:False})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 64\n",
    "kp1 = 0.8\n",
    "kp2 = 0.7\n",
    "kp3 = 0.6\n",
    "kp4 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a single CIFAR-10 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('Checking the Training on a Single Batch...')\n",
    "#with tf.Session() as sess:\n",
    "#    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "#    for epoch in range(epochs):\n",
    "#        batch_i = 1\n",
    "#        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "#            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels, True)\n",
    "#        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "#        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully train the model\n",
    "Deep structure 11 layers, withFliplr, Gaussian Blur, Crop, and Affine, 200 Epochs : 91.1 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     3.6496 Validation Accuracy: 0.095800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.1166 Validation Accuracy: 0.326000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.7831 Validation Accuracy: 0.368400\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.1078 Validation Accuracy: 0.361200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.3064 Validation Accuracy: 0.372200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.8641 Validation Accuracy: 0.409200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.9235 Validation Accuracy: 0.492800\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.3952 Validation Accuracy: 0.494800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.1235 Validation Accuracy: 0.589800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.4607 Validation Accuracy: 0.531600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.0950 Validation Accuracy: 0.592200\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.3730 Validation Accuracy: 0.608000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.8790 Validation Accuracy: 0.638200\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.9764 Validation Accuracy: 0.642000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.1461 Validation Accuracy: 0.657400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.9434 Validation Accuracy: 0.632800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.0963 Validation Accuracy: 0.638800\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.7192 Validation Accuracy: 0.699800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.8192 Validation Accuracy: 0.675600\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.8917 Validation Accuracy: 0.674200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.8023 Validation Accuracy: 0.690600\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.2522 Validation Accuracy: 0.693000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.5916 Validation Accuracy: 0.738600\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.7247 Validation Accuracy: 0.740800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.8698 Validation Accuracy: 0.730800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.0434 Validation Accuracy: 0.714800\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.9448 Validation Accuracy: 0.719400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.7492 Validation Accuracy: 0.704600\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.7140 Validation Accuracy: 0.738400\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.7966 Validation Accuracy: 0.740200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.0485 Validation Accuracy: 0.738000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.9333 Validation Accuracy: 0.741200\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.5198 Validation Accuracy: 0.771800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.8845 Validation Accuracy: 0.777000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.8496 Validation Accuracy: 0.762600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.8734 Validation Accuracy: 0.741000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.7940 Validation Accuracy: 0.777200\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.6088 Validation Accuracy: 0.783800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.5722 Validation Accuracy: 0.765000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.6583 Validation Accuracy: 0.786600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.7961 Validation Accuracy: 0.775800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.5020 Validation Accuracy: 0.767600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.6416 Validation Accuracy: 0.766600\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.4605 Validation Accuracy: 0.793600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.5770 Validation Accuracy: 0.780800\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.9588 Validation Accuracy: 0.766600\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.5884 Validation Accuracy: 0.802000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.5709 Validation Accuracy: 0.782800\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.5535 Validation Accuracy: 0.789000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.6975 Validation Accuracy: 0.800000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.6603 Validation Accuracy: 0.822400\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.7229 Validation Accuracy: 0.800600\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.5018 Validation Accuracy: 0.798600\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.5049 Validation Accuracy: 0.827200\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.6156 Validation Accuracy: 0.781800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.8079 Validation Accuracy: 0.802200\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.6000 Validation Accuracy: 0.814800\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.5989 Validation Accuracy: 0.826600\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.5119 Validation Accuracy: 0.807400\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.3973 Validation Accuracy: 0.830200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.6352 Validation Accuracy: 0.831400\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.7571 Validation Accuracy: 0.823800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.4059 Validation Accuracy: 0.839000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.6051 Validation Accuracy: 0.822400\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.3883 Validation Accuracy: 0.832600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.4569 Validation Accuracy: 0.831200\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.6318 Validation Accuracy: 0.825200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.4116 Validation Accuracy: 0.817000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.4717 Validation Accuracy: 0.805400\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.5948 Validation Accuracy: 0.832400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.5579 Validation Accuracy: 0.816600\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.4323 Validation Accuracy: 0.829600\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.3161 Validation Accuracy: 0.839600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.3325 Validation Accuracy: 0.845000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.3755 Validation Accuracy: 0.843000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.4683 Validation Accuracy: 0.843000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.4651 Validation Accuracy: 0.825600\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.2610 Validation Accuracy: 0.853800\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.3244 Validation Accuracy: 0.854400\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.3074 Validation Accuracy: 0.845400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.4931 Validation Accuracy: 0.846200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.3905 Validation Accuracy: 0.840600\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.4008 Validation Accuracy: 0.861200\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.2623 Validation Accuracy: 0.855000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.2373 Validation Accuracy: 0.858200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.5418 Validation Accuracy: 0.865000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.3364 Validation Accuracy: 0.847000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.2960 Validation Accuracy: 0.844400\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.4169 Validation Accuracy: 0.842600\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.3002 Validation Accuracy: 0.857200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.5925 Validation Accuracy: 0.850200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.2761 Validation Accuracy: 0.849200\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.2708 Validation Accuracy: 0.869000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.2971 Validation Accuracy: 0.860200\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.2406 Validation Accuracy: 0.852200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.4780 Validation Accuracy: 0.856400\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.4203 Validation Accuracy: 0.858600\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.2003 Validation Accuracy: 0.855600\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.3014 Validation Accuracy: 0.852200\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.2861 Validation Accuracy: 0.861600\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.5097 Validation Accuracy: 0.857600\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.2817 Validation Accuracy: 0.855400\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.2232 Validation Accuracy: 0.855600\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.2146 Validation Accuracy: 0.870200\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.2291 Validation Accuracy: 0.862000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.3281 Validation Accuracy: 0.865000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.4065 Validation Accuracy: 0.863600\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.2128 Validation Accuracy: 0.864400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.1932 Validation Accuracy: 0.869800\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.1350 Validation Accuracy: 0.872600\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.5635 Validation Accuracy: 0.863400\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.3513 Validation Accuracy: 0.870600\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.3118 Validation Accuracy: 0.867400\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.1611 Validation Accuracy: 0.876200\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.4357 Validation Accuracy: 0.866600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.5272 Validation Accuracy: 0.864600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.2226 Validation Accuracy: 0.871400\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.2079 Validation Accuracy: 0.870400\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.2379 Validation Accuracy: 0.874800\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.2326 Validation Accuracy: 0.871200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.3752 Validation Accuracy: 0.869400\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.2806 Validation Accuracy: 0.871800\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.1949 Validation Accuracy: 0.856400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.3496 Validation Accuracy: 0.857800\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.2295 Validation Accuracy: 0.868600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.3435 Validation Accuracy: 0.866600\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.2404 Validation Accuracy: 0.867200\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.3641 Validation Accuracy: 0.863000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.2791 Validation Accuracy: 0.868400\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.2630 Validation Accuracy: 0.856000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.4277 Validation Accuracy: 0.869000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.4389 Validation Accuracy: 0.870200\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.2062 Validation Accuracy: 0.876200\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.3059 Validation Accuracy: 0.881800\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.1979 Validation Accuracy: 0.870600\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.3505 Validation Accuracy: 0.864000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.2174 Validation Accuracy: 0.885400\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.1209 Validation Accuracy: 0.882400\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.1983 Validation Accuracy: 0.875000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.3517 Validation Accuracy: 0.881200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.2863 Validation Accuracy: 0.883200\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.2783 Validation Accuracy: 0.874800\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.1302 Validation Accuracy: 0.882600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.1297 Validation Accuracy: 0.879600\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.1923 Validation Accuracy: 0.865000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.2992 Validation Accuracy: 0.879600\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.3054 Validation Accuracy: 0.874000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.2557 Validation Accuracy: 0.881600\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.1105 Validation Accuracy: 0.889000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.1261 Validation Accuracy: 0.888000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.3044 Validation Accuracy: 0.884600\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.3819 Validation Accuracy: 0.884600\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.1171 Validation Accuracy: 0.894400\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.2732 Validation Accuracy: 0.879200\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.1636 Validation Accuracy: 0.882800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.3855 Validation Accuracy: 0.884400\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.2319 Validation Accuracy: 0.885400\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.1771 Validation Accuracy: 0.889800\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.2255 Validation Accuracy: 0.887600\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.1863 Validation Accuracy: 0.883600\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.3996 Validation Accuracy: 0.879000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.2962 Validation Accuracy: 0.886400\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.1845 Validation Accuracy: 0.886200\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.1458 Validation Accuracy: 0.893800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.3022 Validation Accuracy: 0.881200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.2518 Validation Accuracy: 0.885000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.2962 Validation Accuracy: 0.885800\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.1764 Validation Accuracy: 0.872200\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.0844 Validation Accuracy: 0.891600\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.1485 Validation Accuracy: 0.891200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.2731 Validation Accuracy: 0.885800\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.2711 Validation Accuracy: 0.881000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.2353 Validation Accuracy: 0.891400\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.1685 Validation Accuracy: 0.896200\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.2184 Validation Accuracy: 0.880400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.3295 Validation Accuracy: 0.886600\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.2006 Validation Accuracy: 0.863000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.0958 Validation Accuracy: 0.885000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.2039 Validation Accuracy: 0.892600\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.1087 Validation Accuracy: 0.888400\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.2142 Validation Accuracy: 0.886800\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.2021 Validation Accuracy: 0.891000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.0868 Validation Accuracy: 0.890400\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.2419 Validation Accuracy: 0.894000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.0989 Validation Accuracy: 0.891200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.2268 Validation Accuracy: 0.894200\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.2563 Validation Accuracy: 0.896000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.1285 Validation Accuracy: 0.895600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.2026 Validation Accuracy: 0.891200\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.1220 Validation Accuracy: 0.888000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.3180 Validation Accuracy: 0.896400\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.2459 Validation Accuracy: 0.896000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.0641 Validation Accuracy: 0.889600\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.1611 Validation Accuracy: 0.891000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.1179 Validation Accuracy: 0.882800\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.3455 Validation Accuracy: 0.895000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.3241 Validation Accuracy: 0.888400\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.1743 Validation Accuracy: 0.889400\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.0976 Validation Accuracy: 0.893000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.0504 Validation Accuracy: 0.886400\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.1152 Validation Accuracy: 0.896600\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.2214 Validation Accuracy: 0.889400\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.1066 Validation Accuracy: 0.888800\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.1880 Validation Accuracy: 0.887000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.1340 Validation Accuracy: 0.885600\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.3667 Validation Accuracy: 0.887000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.1224 Validation Accuracy: 0.896000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.1135 Validation Accuracy: 0.886600\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.1057 Validation Accuracy: 0.895200\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.0897 Validation Accuracy: 0.884000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.1827 Validation Accuracy: 0.895200\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.1650 Validation Accuracy: 0.882200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.1545 Validation Accuracy: 0.894600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.2703 Validation Accuracy: 0.894800\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.1732 Validation Accuracy: 0.888600\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.3072 Validation Accuracy: 0.894800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.1769 Validation Accuracy: 0.872200\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.0323 Validation Accuracy: 0.895000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.0738 Validation Accuracy: 0.894400\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.1013 Validation Accuracy: 0.884000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.1348 Validation Accuracy: 0.893200\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.2102 Validation Accuracy: 0.888800\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.1247 Validation Accuracy: 0.890600\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.2195 Validation Accuracy: 0.898000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.1406 Validation Accuracy: 0.894800\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.1236 Validation Accuracy: 0.898000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.0993 Validation Accuracy: 0.901400\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.0882 Validation Accuracy: 0.891800\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.0832 Validation Accuracy: 0.902000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.1481 Validation Accuracy: 0.886200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.2457 Validation Accuracy: 0.899600\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.2277 Validation Accuracy: 0.896600\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.0589 Validation Accuracy: 0.896000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.2174 Validation Accuracy: 0.894000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.1733 Validation Accuracy: 0.894800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.3309 Validation Accuracy: 0.899600\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.1250 Validation Accuracy: 0.900000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.0578 Validation Accuracy: 0.892200\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.1169 Validation Accuracy: 0.901200\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.0819 Validation Accuracy: 0.888200\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.1276 Validation Accuracy: 0.899800\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.1408 Validation Accuracy: 0.895000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.1087 Validation Accuracy: 0.894000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.1393 Validation Accuracy: 0.896400\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.1106 Validation Accuracy: 0.895000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.2086 Validation Accuracy: 0.895200\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.1954 Validation Accuracy: 0.902800\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.0549 Validation Accuracy: 0.898600\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.1697 Validation Accuracy: 0.900000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.0634 Validation Accuracy: 0.891600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.0772 Validation Accuracy: 0.900800\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.2188 Validation Accuracy: 0.894600\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.0292 Validation Accuracy: 0.899200\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.0798 Validation Accuracy: 0.895200\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.1678 Validation Accuracy: 0.896000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.0581 Validation Accuracy: 0.903600\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.0388 Validation Accuracy: 0.900000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.2274 Validation Accuracy: 0.887600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.1840 Validation Accuracy: 0.896000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.0371 Validation Accuracy: 0.897000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.3094 Validation Accuracy: 0.898200\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.1937 Validation Accuracy: 0.904600\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.1126 Validation Accuracy: 0.894200\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.1105 Validation Accuracy: 0.891600\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.0425 Validation Accuracy: 0.900200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.1318 Validation Accuracy: 0.898200\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.1715 Validation Accuracy: 0.901800\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.0510 Validation Accuracy: 0.894000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.2736 Validation Accuracy: 0.893600\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.0559 Validation Accuracy: 0.892800\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.1606 Validation Accuracy: 0.899600\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.0835 Validation Accuracy: 0.899800\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.1752 Validation Accuracy: 0.896800\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.0850 Validation Accuracy: 0.901000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.0337 Validation Accuracy: 0.896000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.1223 Validation Accuracy: 0.898400\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.1100 Validation Accuracy: 0.901200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.1171 Validation Accuracy: 0.895400\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.1259 Validation Accuracy: 0.904800\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.0806 Validation Accuracy: 0.902400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.1199 Validation Accuracy: 0.898400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.1163 Validation Accuracy: 0.900600\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.0775 Validation Accuracy: 0.892400\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.1223 Validation Accuracy: 0.896800\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.0958 Validation Accuracy: 0.900600\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.0774 Validation Accuracy: 0.892000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.2117 Validation Accuracy: 0.902200\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.1436 Validation Accuracy: 0.895000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.0975 Validation Accuracy: 0.899200\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.0432 Validation Accuracy: 0.902200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.0637 Validation Accuracy: 0.896200\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.2604 Validation Accuracy: 0.902400\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.1284 Validation Accuracy: 0.896600\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.0566 Validation Accuracy: 0.895800\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.0363 Validation Accuracy: 0.902200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.2145 Validation Accuracy: 0.897400\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.1355 Validation Accuracy: 0.899000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.0685 Validation Accuracy: 0.894800\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.2300 Validation Accuracy: 0.894000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.0843 Validation Accuracy: 0.894000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.0553 Validation Accuracy: 0.901400\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.1042 Validation Accuracy: 0.894400\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.1018 Validation Accuracy: 0.900200\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.0745 Validation Accuracy: 0.902600\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.0331 Validation Accuracy: 0.900800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.1986 Validation Accuracy: 0.896600\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.1397 Validation Accuracy: 0.900800\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.0252 Validation Accuracy: 0.902800\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.0587 Validation Accuracy: 0.897600\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.0701 Validation Accuracy: 0.899800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.0753 Validation Accuracy: 0.904600\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.1135 Validation Accuracy: 0.903400\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.0398 Validation Accuracy: 0.896200\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.0460 Validation Accuracy: 0.902000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.1579 Validation Accuracy: 0.901400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.1496 Validation Accuracy: 0.899800\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.0805 Validation Accuracy: 0.906000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.0287 Validation Accuracy: 0.899200\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.0890 Validation Accuracy: 0.901800\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.0931 Validation Accuracy: 0.899000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.1330 Validation Accuracy: 0.903000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.1999 Validation Accuracy: 0.903800\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.1075 Validation Accuracy: 0.900800\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.2012 Validation Accuracy: 0.902200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.0846 Validation Accuracy: 0.903600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.1345 Validation Accuracy: 0.903400\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.1280 Validation Accuracy: 0.893600\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.0399 Validation Accuracy: 0.901600\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.0550 Validation Accuracy: 0.897200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.0385 Validation Accuracy: 0.900800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.1077 Validation Accuracy: 0.901600\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.1940 Validation Accuracy: 0.913600\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.0631 Validation Accuracy: 0.901400\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.0989 Validation Accuracy: 0.904600\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.0759 Validation Accuracy: 0.904600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0689 Validation Accuracy: 0.902400\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.0964 Validation Accuracy: 0.907000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.0142 Validation Accuracy: 0.906800\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.1371 Validation Accuracy: 0.902000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.0415 Validation Accuracy: 0.905200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.1207 Validation Accuracy: 0.905800\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.1134 Validation Accuracy: 0.904200\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.1070 Validation Accuracy: 0.900000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.0301 Validation Accuracy: 0.901600\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.0155 Validation Accuracy: 0.899400\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.2429 Validation Accuracy: 0.907200\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.1613 Validation Accuracy: 0.898600\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.0508 Validation Accuracy: 0.901600\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.1337 Validation Accuracy: 0.903000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.0681 Validation Accuracy: 0.894600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.1531 Validation Accuracy: 0.903600\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.1409 Validation Accuracy: 0.903000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.1129 Validation Accuracy: 0.897600\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.1311 Validation Accuracy: 0.900400\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.0692 Validation Accuracy: 0.901400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.1233 Validation Accuracy: 0.894600\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.1410 Validation Accuracy: 0.897000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.1293 Validation Accuracy: 0.905400\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.0521 Validation Accuracy: 0.902800\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.0203 Validation Accuracy: 0.905200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.2056 Validation Accuracy: 0.901400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.1425 Validation Accuracy: 0.906000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.0823 Validation Accuracy: 0.902400\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.1229 Validation Accuracy: 0.900000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.0355 Validation Accuracy: 0.899400\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0377 Validation Accuracy: 0.906400\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.1770 Validation Accuracy: 0.899400\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.0632 Validation Accuracy: 0.904800\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.1217 Validation Accuracy: 0.904600\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.1436 Validation Accuracy: 0.899000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0785 Validation Accuracy: 0.907600\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.1231 Validation Accuracy: 0.903400\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.0553 Validation Accuracy: 0.906000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.0533 Validation Accuracy: 0.907200\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.0127 Validation Accuracy: 0.906600\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.2488 Validation Accuracy: 0.898400\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.0522 Validation Accuracy: 0.905800\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.0422 Validation Accuracy: 0.902600\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.0258 Validation Accuracy: 0.900600\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.2313 Validation Accuracy: 0.900800\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.1141 Validation Accuracy: 0.905800\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.1899 Validation Accuracy: 0.907600\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.0335 Validation Accuracy: 0.908800\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.0123 Validation Accuracy: 0.902200\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.0733 Validation Accuracy: 0.903600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.1522 Validation Accuracy: 0.908800\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.0970 Validation Accuracy: 0.907600\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.0698 Validation Accuracy: 0.903200\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.1046 Validation Accuracy: 0.905400\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.1011 Validation Accuracy: 0.904000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.1303 Validation Accuracy: 0.904600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.2150 Validation Accuracy: 0.898000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.0159 Validation Accuracy: 0.909000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.1585 Validation Accuracy: 0.904400\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.0275 Validation Accuracy: 0.904600\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0407 Validation Accuracy: 0.899000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.1362 Validation Accuracy: 0.898200\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.0330 Validation Accuracy: 0.905000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.0998 Validation Accuracy: 0.901600\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.0294 Validation Accuracy: 0.906400\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0345 Validation Accuracy: 0.900800\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.1251 Validation Accuracy: 0.899400\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.0343 Validation Accuracy: 0.904800\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.0902 Validation Accuracy: 0.905400\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.1836 Validation Accuracy: 0.893600\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.1532 Validation Accuracy: 0.911000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.0746 Validation Accuracy: 0.910200\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.1210 Validation Accuracy: 0.900400\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.0303 Validation Accuracy: 0.903000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.0867 Validation Accuracy: 0.904200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.1715 Validation Accuracy: 0.905000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.2373 Validation Accuracy: 0.904800\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.1135 Validation Accuracy: 0.901200\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.0505 Validation Accuracy: 0.908400\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.0611 Validation Accuracy: 0.910400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0850 Validation Accuracy: 0.909200\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.1244 Validation Accuracy: 0.900600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.0255 Validation Accuracy: 0.906800\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.0757 Validation Accuracy: 0.901400\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.0340 Validation Accuracy: 0.906200\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.2526 Validation Accuracy: 0.901400\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.0730 Validation Accuracy: 0.912600\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.1244 Validation Accuracy: 0.905400\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.1166 Validation Accuracy: 0.902000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.0251 Validation Accuracy: 0.904400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0594 Validation Accuracy: 0.906600\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.0555 Validation Accuracy: 0.904800\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.0561 Validation Accuracy: 0.905600\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.0549 Validation Accuracy: 0.904800\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.0424 Validation Accuracy: 0.896800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.2720 Validation Accuracy: 0.900400\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.1399 Validation Accuracy: 0.907600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.0148 Validation Accuracy: 0.900800\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.1153 Validation Accuracy: 0.907400\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.0300 Validation Accuracy: 0.903600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0833 Validation Accuracy: 0.895400\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.1674 Validation Accuracy: 0.906200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.0299 Validation Accuracy: 0.897600\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.1061 Validation Accuracy: 0.903200\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.0582 Validation Accuracy: 0.899400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0523 Validation Accuracy: 0.905600\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.0170 Validation Accuracy: 0.905600\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.0244 Validation Accuracy: 0.905000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.0384 Validation Accuracy: 0.908200\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.0065 Validation Accuracy: 0.903400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0319 Validation Accuracy: 0.902000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.0766 Validation Accuracy: 0.909400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.0156 Validation Accuracy: 0.904600\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.0794 Validation Accuracy: 0.903000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.1035 Validation Accuracy: 0.901400\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.1558 Validation Accuracy: 0.908000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.1040 Validation Accuracy: 0.906600\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.1034 Validation Accuracy: 0.902600\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.0141 Validation Accuracy: 0.897000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.0119 Validation Accuracy: 0.902200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.2686 Validation Accuracy: 0.905800\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.2539 Validation Accuracy: 0.902000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.0279 Validation Accuracy: 0.906200\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.0704 Validation Accuracy: 0.903200\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.0167 Validation Accuracy: 0.901400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.1639 Validation Accuracy: 0.912000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.1406 Validation Accuracy: 0.905000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.0306 Validation Accuracy: 0.902600\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.0256 Validation Accuracy: 0.903800\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.0286 Validation Accuracy: 0.903400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0549 Validation Accuracy: 0.909600\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.0787 Validation Accuracy: 0.907200\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.0515 Validation Accuracy: 0.895800\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.0469 Validation Accuracy: 0.906000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.0396 Validation Accuracy: 0.909600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0907 Validation Accuracy: 0.908400\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.0862 Validation Accuracy: 0.901800\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.0040 Validation Accuracy: 0.897800\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.0510 Validation Accuracy: 0.909400\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.0679 Validation Accuracy: 0.907800\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0161 Validation Accuracy: 0.909600\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.1213 Validation Accuracy: 0.910800\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.0287 Validation Accuracy: 0.901200\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.0108 Validation Accuracy: 0.907000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.1037 Validation Accuracy: 0.910800\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.1464 Validation Accuracy: 0.910400\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.0872 Validation Accuracy: 0.904800\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.0524 Validation Accuracy: 0.904600\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.1002 Validation Accuracy: 0.908800\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.0291 Validation Accuracy: 0.905400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.2023 Validation Accuracy: 0.903600\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.2282 Validation Accuracy: 0.903400\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.0537 Validation Accuracy: 0.896400\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.0366 Validation Accuracy: 0.903600\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.1348 Validation Accuracy: 0.901600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0896 Validation Accuracy: 0.905200\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.1893 Validation Accuracy: 0.907800\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.0036 Validation Accuracy: 0.901600\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.0822 Validation Accuracy: 0.904000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.0670 Validation Accuracy: 0.905800\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0715 Validation Accuracy: 0.907800\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.0689 Validation Accuracy: 0.902400\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.0105 Validation Accuracy: 0.902200\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.1083 Validation Accuracy: 0.901200\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.0730 Validation Accuracy: 0.903800\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.1621 Validation Accuracy: 0.906000\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.0476 Validation Accuracy: 0.907600\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.0321 Validation Accuracy: 0.904600\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.0485 Validation Accuracy: 0.905600\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.0144 Validation Accuracy: 0.905000\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.0401 Validation Accuracy: 0.907000\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.0677 Validation Accuracy: 0.910600\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.0018 Validation Accuracy: 0.907400\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.0275 Validation Accuracy: 0.903600\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.0060 Validation Accuracy: 0.908000\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.1229 Validation Accuracy: 0.907000\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.0560 Validation Accuracy: 0.912200\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.1142 Validation Accuracy: 0.909800\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.0768 Validation Accuracy: 0.903800\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.0295 Validation Accuracy: 0.905000\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.0429 Validation Accuracy: 0.905200\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.0317 Validation Accuracy: 0.907800\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.0015 Validation Accuracy: 0.906000\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.0543 Validation Accuracy: 0.907400\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.0126 Validation Accuracy: 0.899000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.2108 Validation Accuracy: 0.904200\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.0890 Validation Accuracy: 0.911000\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.0013 Validation Accuracy: 0.904200\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.0493 Validation Accuracy: 0.904800\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.0455 Validation Accuracy: 0.909600\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.0065 Validation Accuracy: 0.906400\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.1263 Validation Accuracy: 0.906400\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.0219 Validation Accuracy: 0.910800\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.0291 Validation Accuracy: 0.913200\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.0217 Validation Accuracy: 0.907800\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.1582 Validation Accuracy: 0.911800\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.0554 Validation Accuracy: 0.909600\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.0691 Validation Accuracy: 0.912400\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.0839 Validation Accuracy: 0.909600\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.0087 Validation Accuracy: 0.909200\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.1021 Validation Accuracy: 0.907600\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.0432 Validation Accuracy: 0.910400\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.0932 Validation Accuracy: 0.904400\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.0977 Validation Accuracy: 0.906800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.0208 Validation Accuracy: 0.906600\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.0499 Validation Accuracy: 0.902800\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.0451 Validation Accuracy: 0.905400\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.0301 Validation Accuracy: 0.904600\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.0705 Validation Accuracy: 0.903400\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.0113 Validation Accuracy: 0.908400\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.0341 Validation Accuracy: 0.914200\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.3154 Validation Accuracy: 0.910600\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.0105 Validation Accuracy: 0.913600\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.0498 Validation Accuracy: 0.902600\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.0616 Validation Accuracy: 0.901200\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.0359 Validation Accuracy: 0.909800\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.0354 Validation Accuracy: 0.908600\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.0227 Validation Accuracy: 0.905800\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.0526 Validation Accuracy: 0.910000\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.0357 Validation Accuracy: 0.907200\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.0382 Validation Accuracy: 0.897200\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.0460 Validation Accuracy: 0.910400\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.0136 Validation Accuracy: 0.907000\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.0167 Validation Accuracy: 0.907000\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.0098 Validation Accuracy: 0.902200\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.0565 Validation Accuracy: 0.901600\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.0259 Validation Accuracy: 0.905400\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.0036 Validation Accuracy: 0.906800\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.0182 Validation Accuracy: 0.909000\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.0150 Validation Accuracy: 0.912000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.0622 Validation Accuracy: 0.909200\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.1658 Validation Accuracy: 0.906400\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.0509 Validation Accuracy: 0.907600\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.0121 Validation Accuracy: 0.905800\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.0365 Validation Accuracy: 0.905800\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.0198 Validation Accuracy: 0.908200\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.0187 Validation Accuracy: 0.905000\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.0084 Validation Accuracy: 0.908200\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.0106 Validation Accuracy: 0.904800\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.0412 Validation Accuracy: 0.905400\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.1550 Validation Accuracy: 0.897800\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.1363 Validation Accuracy: 0.908800\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.0277 Validation Accuracy: 0.896400\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.0686 Validation Accuracy: 0.908400\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.1142 Validation Accuracy: 0.907200\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.0463 Validation Accuracy: 0.908200\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.2332 Validation Accuracy: 0.906000\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.0151 Validation Accuracy: 0.906800\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.0249 Validation Accuracy: 0.907800\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.0505 Validation Accuracy: 0.908800\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.0365 Validation Accuracy: 0.910200\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.0041 Validation Accuracy: 0.897400\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.0015 Validation Accuracy: 0.912000\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.0464 Validation Accuracy: 0.909800\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.0583 Validation Accuracy: 0.908000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.2034 Validation Accuracy: 0.909400\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.0059 Validation Accuracy: 0.913000\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.0588 Validation Accuracy: 0.906800\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.0126 Validation Accuracy: 0.906400\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.1164 Validation Accuracy: 0.901200\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.0361 Validation Accuracy: 0.910200\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.1950 Validation Accuracy: 0.904200\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.0304 Validation Accuracy: 0.903600\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.0151 Validation Accuracy: 0.902000\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.0470 Validation Accuracy: 0.909000\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.1010 Validation Accuracy: 0.910400\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.0213 Validation Accuracy: 0.909000\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.0554 Validation Accuracy: 0.902400\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.0739 Validation Accuracy: 0.906800\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.0702 Validation Accuracy: 0.909800\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.0573 Validation Accuracy: 0.907000\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.0511 Validation Accuracy: 0.907600\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.0488 Validation Accuracy: 0.907400\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.0306 Validation Accuracy: 0.911000\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.0518 Validation Accuracy: 0.903000\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.0335 Validation Accuracy: 0.911400\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.2430 Validation Accuracy: 0.907400\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.0056 Validation Accuracy: 0.904200\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.0914 Validation Accuracy: 0.905600\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.0418 Validation Accuracy: 0.909400\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.0302 Validation Accuracy: 0.905200\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.0122 Validation Accuracy: 0.907200\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.0415 Validation Accuracy: 0.910800\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.0558 Validation Accuracy: 0.905600\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.1105 Validation Accuracy: 0.904200\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.0295 Validation Accuracy: 0.909200\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.1946 Validation Accuracy: 0.910800\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.0078 Validation Accuracy: 0.897600\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.0764 Validation Accuracy: 0.904200\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.0312 Validation Accuracy: 0.904000\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.0131 Validation Accuracy: 0.909800\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.0541 Validation Accuracy: 0.909400\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.0087 Validation Accuracy: 0.907800\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.0366 Validation Accuracy: 0.907800\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.0067 Validation Accuracy: 0.905200\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.0402 Validation Accuracy: 0.908200\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.1684 Validation Accuracy: 0.907000\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.0035 Validation Accuracy: 0.907000\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.0224 Validation Accuracy: 0.908200\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.0612 Validation Accuracy: 0.908000\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.1843 Validation Accuracy: 0.910200\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.0732 Validation Accuracy: 0.907000\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.0588 Validation Accuracy: 0.907000\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.1146 Validation Accuracy: 0.903400\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.0200 Validation Accuracy: 0.907200\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.2831 Validation Accuracy: 0.912000\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.0776 Validation Accuracy: 0.907200\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.0225 Validation Accuracy: 0.910400\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.0242 Validation Accuracy: 0.904200\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.0062 Validation Accuracy: 0.914200\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.1946 Validation Accuracy: 0.910400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.0319 Validation Accuracy: 0.909600\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.0637 Validation Accuracy: 0.908400\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.0649 Validation Accuracy: 0.906200\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.0329 Validation Accuracy: 0.909400\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.0686 Validation Accuracy: 0.908000\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.0837 Validation Accuracy: 0.905000\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.0208 Validation Accuracy: 0.907000\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.0708 Validation Accuracy: 0.906000\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.0037 Validation Accuracy: 0.911600\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.0254 Validation Accuracy: 0.910600\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.0147 Validation Accuracy: 0.908200\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.0377 Validation Accuracy: 0.907000\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.0972 Validation Accuracy: 0.908200\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.0252 Validation Accuracy: 0.906000\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.1260 Validation Accuracy: 0.907600\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.0170 Validation Accuracy: 0.904600\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.0845 Validation Accuracy: 0.905000\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.1628 Validation Accuracy: 0.908000\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.0563 Validation Accuracy: 0.906400\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.1006 Validation Accuracy: 0.906000\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.0405 Validation Accuracy: 0.906400\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.1168 Validation Accuracy: 0.902800\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.0384 Validation Accuracy: 0.905600\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.0220 Validation Accuracy: 0.908400\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.0668 Validation Accuracy: 0.909000\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.1753 Validation Accuracy: 0.909400\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.0211 Validation Accuracy: 0.903600\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.0251 Validation Accuracy: 0.903600\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.0957 Validation Accuracy: 0.905800\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.0825 Validation Accuracy: 0.909400\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.1632 Validation Accuracy: 0.906200\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.0529 Validation Accuracy: 0.909800\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.0372 Validation Accuracy: 0.908200\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.0267 Validation Accuracy: 0.903800\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.1607 Validation Accuracy: 0.907200\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.0271 Validation Accuracy: 0.905200\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.0328 Validation Accuracy: 0.909000\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.0357 Validation Accuracy: 0.904200\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.0161 Validation Accuracy: 0.904200\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.0469 Validation Accuracy: 0.913200\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.0380 Validation Accuracy: 0.909400\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.0082 Validation Accuracy: 0.905200\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.0149 Validation Accuracy: 0.904000\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.0101 Validation Accuracy: 0.902400\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.2250 Validation Accuracy: 0.910600\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.0238 Validation Accuracy: 0.908000\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.0628 Validation Accuracy: 0.904200\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.0513 Validation Accuracy: 0.905400\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.0084 Validation Accuracy: 0.903200\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.1184 Validation Accuracy: 0.906400\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.0188 Validation Accuracy: 0.906800\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.0914 Validation Accuracy: 0.906400\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.1353 Validation Accuracy: 0.910200\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.0070 Validation Accuracy: 0.906400\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.0108 Validation Accuracy: 0.905200\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.1278 Validation Accuracy: 0.910400\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.0113 Validation Accuracy: 0.907000\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.0424 Validation Accuracy: 0.911000\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.1422 Validation Accuracy: 0.897200\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.0124 Validation Accuracy: 0.908400\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.0205 Validation Accuracy: 0.910400\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.0050 Validation Accuracy: 0.909400\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.0911 Validation Accuracy: 0.905000\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.0352 Validation Accuracy: 0.902200\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.0323 Validation Accuracy: 0.908000\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.1775 Validation Accuracy: 0.909200\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.0179 Validation Accuracy: 0.911600\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.0078 Validation Accuracy: 0.913000\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.0106 Validation Accuracy: 0.908600\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.0865 Validation Accuracy: 0.904000\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.0418 Validation Accuracy: 0.907200\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.0419 Validation Accuracy: 0.908600\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.0525 Validation Accuracy: 0.911000\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.1333 Validation Accuracy: 0.906800\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.1375 Validation Accuracy: 0.901800\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.0174 Validation Accuracy: 0.910400\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.0563 Validation Accuracy: 0.906000\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.0747 Validation Accuracy: 0.909000\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.0134 Validation Accuracy: 0.909600\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.0324 Validation Accuracy: 0.912400\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.0225 Validation Accuracy: 0.907800\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.0287 Validation Accuracy: 0.905000\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.0297 Validation Accuracy: 0.912800\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.0085 Validation Accuracy: 0.906000\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.0117 Validation Accuracy: 0.910800\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.0533 Validation Accuracy: 0.910000\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.0239 Validation Accuracy: 0.909200\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.1328 Validation Accuracy: 0.913600\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.0080 Validation Accuracy: 0.911000\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.1686 Validation Accuracy: 0.899800\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.0258 Validation Accuracy: 0.908600\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.0644 Validation Accuracy: 0.907000\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.0193 Validation Accuracy: 0.905800\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.0458 Validation Accuracy: 0.913800\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.0484 Validation Accuracy: 0.913800\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.0561 Validation Accuracy: 0.917400\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.0123 Validation Accuracy: 0.911400\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.0216 Validation Accuracy: 0.903000\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.0553 Validation Accuracy: 0.906600\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.0474 Validation Accuracy: 0.909600\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.0324 Validation Accuracy: 0.910400\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.0011 Validation Accuracy: 0.909200\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.0508 Validation Accuracy: 0.913000\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.1099 Validation Accuracy: 0.906000\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.0047 Validation Accuracy: 0.909600\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.0450 Validation Accuracy: 0.910400\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.0060 Validation Accuracy: 0.910200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.1442 Validation Accuracy: 0.912200\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.0485 Validation Accuracy: 0.903400\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.0886 Validation Accuracy: 0.908600\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.1858 Validation Accuracy: 0.912200\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.0014 Validation Accuracy: 0.907800\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.0344 Validation Accuracy: 0.908600\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.0278 Validation Accuracy: 0.909800\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.0249 Validation Accuracy: 0.910800\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.0510 Validation Accuracy: 0.914800\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.0115 Validation Accuracy: 0.900800\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.0096 Validation Accuracy: 0.911800\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.0274 Validation Accuracy: 0.905600\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.1132 Validation Accuracy: 0.912200\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.0784 Validation Accuracy: 0.904200\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.0467 Validation Accuracy: 0.905400\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.0087 Validation Accuracy: 0.909400\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.0539 Validation Accuracy: 0.909600\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.1244 Validation Accuracy: 0.909200\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.0989 Validation Accuracy: 0.914000\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.0368 Validation Accuracy: 0.903200\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.0295 Validation Accuracy: 0.913000\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.0113 Validation Accuracy: 0.908600\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.0210 Validation Accuracy: 0.913800\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.2761 Validation Accuracy: 0.910400\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.0038 Validation Accuracy: 0.911200\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.0070 Validation Accuracy: 0.911600\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.0797 Validation Accuracy: 0.909200\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.0780 Validation Accuracy: 0.910800\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.0288 Validation Accuracy: 0.914800\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.0215 Validation Accuracy: 0.901200\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.0100 Validation Accuracy: 0.907800\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.0060 Validation Accuracy: 0.911800\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.0278 Validation Accuracy: 0.911000\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.2336 Validation Accuracy: 0.906600\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.0089 Validation Accuracy: 0.910200\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.0163 Validation Accuracy: 0.904000\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.0471 Validation Accuracy: 0.906200\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.0858 Validation Accuracy: 0.908200\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.0197 Validation Accuracy: 0.911600\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.0039 Validation Accuracy: 0.910000\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.0038 Validation Accuracy: 0.907800\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.0921 Validation Accuracy: 0.907400\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.1280 Validation Accuracy: 0.905600\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.0315 Validation Accuracy: 0.912600\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.0646 Validation Accuracy: 0.905600\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.0129 Validation Accuracy: 0.908600\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.0196 Validation Accuracy: 0.905800\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.0188 Validation Accuracy: 0.911800\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.0128 Validation Accuracy: 0.907600\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.0152 Validation Accuracy: 0.902400\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.1250 Validation Accuracy: 0.909800\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.0084 Validation Accuracy: 0.910600\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.1781 Validation Accuracy: 0.907600\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.0145 Validation Accuracy: 0.909000\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.0045 Validation Accuracy: 0.910800\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.0489 Validation Accuracy: 0.909600\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.0134 Validation Accuracy: 0.908600\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.0476 Validation Accuracy: 0.908200\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.0121 Validation Accuracy: 0.908800\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.0268 Validation Accuracy: 0.911000\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.0687 Validation Accuracy: 0.906400\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.0191 Validation Accuracy: 0.906800\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.1037 Validation Accuracy: 0.915200\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.0203 Validation Accuracy: 0.911600\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.0291 Validation Accuracy: 0.911200\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.0419 Validation Accuracy: 0.905400\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.0391 Validation Accuracy: 0.898400\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.0276 Validation Accuracy: 0.906200\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.0752 Validation Accuracy: 0.912000\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.0499 Validation Accuracy: 0.907800\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.0030 Validation Accuracy: 0.908800\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.0146 Validation Accuracy: 0.914000\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.0436 Validation Accuracy: 0.911200\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.0514 Validation Accuracy: 0.910600\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.0110 Validation Accuracy: 0.910000\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.0461 Validation Accuracy: 0.911400\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.0408 Validation Accuracy: 0.908600\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.0396 Validation Accuracy: 0.910200\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.1046 Validation Accuracy: 0.911800\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.0048 Validation Accuracy: 0.911200\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.0103 Validation Accuracy: 0.910600\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.0027 Validation Accuracy: 0.909200\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.0782 Validation Accuracy: 0.908600\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.0721 Validation Accuracy: 0.907600\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.0094 Validation Accuracy: 0.913400\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.0355 Validation Accuracy: 0.910000\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.0097 Validation Accuracy: 0.908000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.0601 Validation Accuracy: 0.906000\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.0297 Validation Accuracy: 0.905000\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.0029 Validation Accuracy: 0.903000\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.0069 Validation Accuracy: 0.911600\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.0172 Validation Accuracy: 0.907600\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.0562 Validation Accuracy: 0.909400\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.0117 Validation Accuracy: 0.911600\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.0041 Validation Accuracy: 0.908000\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.0660 Validation Accuracy: 0.911000\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.0670 Validation Accuracy: 0.910400\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.0058 Validation Accuracy: 0.908600\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.0048 Validation Accuracy: 0.907600\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.0057 Validation Accuracy: 0.910800\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.0076 Validation Accuracy: 0.912600\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.0060 Validation Accuracy: 0.902000\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.1231 Validation Accuracy: 0.906400\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.0547 Validation Accuracy: 0.911600\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.0168 Validation Accuracy: 0.906200\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.0293 Validation Accuracy: 0.911600\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.0316 Validation Accuracy: 0.903200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.0430 Validation Accuracy: 0.905000\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.0275 Validation Accuracy: 0.906400\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.0049 Validation Accuracy: 0.910400\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.0265 Validation Accuracy: 0.909800\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.0321 Validation Accuracy: 0.912400\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.0912 Validation Accuracy: 0.908600\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.0415 Validation Accuracy: 0.911400\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.0064 Validation Accuracy: 0.908200\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.0660 Validation Accuracy: 0.906200\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.0480 Validation Accuracy: 0.910600\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.0117 Validation Accuracy: 0.907800\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.0183 Validation Accuracy: 0.914000\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.0093 Validation Accuracy: 0.907200\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.0060 Validation Accuracy: 0.911400\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.0037 Validation Accuracy: 0.904000\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.0108 Validation Accuracy: 0.911600\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.0509 Validation Accuracy: 0.910200\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.0188 Validation Accuracy: 0.906600\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.0343 Validation Accuracy: 0.902000\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.0147 Validation Accuracy: 0.905200\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.1008 Validation Accuracy: 0.910200\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.0730 Validation Accuracy: 0.909000\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.0304 Validation Accuracy: 0.910000\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.0049 Validation Accuracy: 0.907200\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.0298 Validation Accuracy: 0.909000\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.0586 Validation Accuracy: 0.905600\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.0110 Validation Accuracy: 0.909800\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.0356 Validation Accuracy: 0.902000\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.0434 Validation Accuracy: 0.905800\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.0107 Validation Accuracy: 0.902600\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.0425 Validation Accuracy: 0.911800\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.0111 Validation Accuracy: 0.905800\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.0059 Validation Accuracy: 0.906800\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.0209 Validation Accuracy: 0.904400\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.0242 Validation Accuracy: 0.910400\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.0356 Validation Accuracy: 0.906000\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.0557 Validation Accuracy: 0.905200\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.0285 Validation Accuracy: 0.908000\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.0161 Validation Accuracy: 0.904400\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.0046 Validation Accuracy: 0.904800\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.0826 Validation Accuracy: 0.904200\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.0248 Validation Accuracy: 0.908200\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.0059 Validation Accuracy: 0.910600\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.0128 Validation Accuracy: 0.911600\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.0022 Validation Accuracy: 0.912000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.0264 Validation Accuracy: 0.908600\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.0161 Validation Accuracy: 0.910000\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.0030 Validation Accuracy: 0.908000\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.0264 Validation Accuracy: 0.906800\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.0240 Validation Accuracy: 0.912200\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.0934 Validation Accuracy: 0.911600\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.0070 Validation Accuracy: 0.907200\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.0256 Validation Accuracy: 0.912200\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.0220 Validation Accuracy: 0.912400\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.0109 Validation Accuracy: 0.912000\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.0136 Validation Accuracy: 0.913000\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.0529 Validation Accuracy: 0.907600\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.0404 Validation Accuracy: 0.907800\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.0188 Validation Accuracy: 0.914600\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.0164 Validation Accuracy: 0.905800\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.0354 Validation Accuracy: 0.914200\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.0680 Validation Accuracy: 0.912400\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.0028 Validation Accuracy: 0.914400\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.0283 Validation Accuracy: 0.908000\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.0561 Validation Accuracy: 0.907800\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.0040 Validation Accuracy: 0.913800\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.0143 Validation Accuracy: 0.909600\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.0719 Validation Accuracy: 0.907800\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.0715 Validation Accuracy: 0.909000\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.1353 Validation Accuracy: 0.910200\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.1356 Validation Accuracy: 0.909200\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.0354 Validation Accuracy: 0.908800\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.0428 Validation Accuracy: 0.900800\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.0180 Validation Accuracy: 0.914600\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.0154 Validation Accuracy: 0.911000\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.0103 Validation Accuracy: 0.916600\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.0900 Validation Accuracy: 0.910800\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.0108 Validation Accuracy: 0.901400\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.0427 Validation Accuracy: 0.907000\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.0158 Validation Accuracy: 0.911400\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.0401 Validation Accuracy: 0.907000\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.0683 Validation Accuracy: 0.907600\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.0123 Validation Accuracy: 0.905000\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.2722 Validation Accuracy: 0.905400\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.0565 Validation Accuracy: 0.905000\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.0508 Validation Accuracy: 0.907400\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.0064 Validation Accuracy: 0.909000\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.0032 Validation Accuracy: 0.910800\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.0188 Validation Accuracy: 0.908800\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.0202 Validation Accuracy: 0.909400\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.0719 Validation Accuracy: 0.908000\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.1021 Validation Accuracy: 0.908200\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.0113 Validation Accuracy: 0.909000\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.0194 Validation Accuracy: 0.907800\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.0182 Validation Accuracy: 0.907200\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.0620 Validation Accuracy: 0.912200\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.0641 Validation Accuracy: 0.910800\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.0232 Validation Accuracy: 0.912400\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.1279 Validation Accuracy: 0.915600\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.0495 Validation Accuracy: 0.906400\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.0083 Validation Accuracy: 0.909600\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.0780 Validation Accuracy: 0.909600\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.0347 Validation Accuracy: 0.913400\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.0069 Validation Accuracy: 0.906000\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.0324 Validation Accuracy: 0.898600\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.1828 Validation Accuracy: 0.907200\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.1284 Validation Accuracy: 0.911000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.0178 Validation Accuracy: 0.910200\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.0261 Validation Accuracy: 0.908600\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.0089 Validation Accuracy: 0.911000\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.0065 Validation Accuracy: 0.900000\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.0563 Validation Accuracy: 0.909000\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.0288 Validation Accuracy: 0.908400\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.0052 Validation Accuracy: 0.907000\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.0032 Validation Accuracy: 0.911800\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.0478 Validation Accuracy: 0.905400\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.0229 Validation Accuracy: 0.904400\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.0175 Validation Accuracy: 0.907000\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.0265 Validation Accuracy: 0.910600\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.0141 Validation Accuracy: 0.908400\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.0109 Validation Accuracy: 0.910200\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.0540 Validation Accuracy: 0.907000\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.0624 Validation Accuracy: 0.903000\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.0673 Validation Accuracy: 0.911000\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.0089 Validation Accuracy: 0.906200\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.0839 Validation Accuracy: 0.906600\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.0129 Validation Accuracy: 0.909200\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.0049 Validation Accuracy: 0.907200\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.0149 Validation Accuracy: 0.907600\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.0098 Validation Accuracy: 0.910200\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.1066 Validation Accuracy: 0.909000\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.0631 Validation Accuracy: 0.910800\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.1307 Validation Accuracy: 0.904200\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.0033 Validation Accuracy: 0.910000\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.0016 Validation Accuracy: 0.909000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.0808 Validation Accuracy: 0.910200\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.0653 Validation Accuracy: 0.908600\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.0162 Validation Accuracy: 0.906200\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.0317 Validation Accuracy: 0.910400\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.1294 Validation Accuracy: 0.907400\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                \n",
    "                seq = iaa.Sequential([iaa.Fliplr(0.5),\n",
    "                                      iaa.GaussianBlur(0, 3.0),\n",
    "                                      iaa.Crop(px=(0, 2)),\n",
    "                                      iaa.Sometimes(0.5, iaa.Affine(rotate=(-45, 45),\n",
    "                                                                    shear=(-16, 16),\n",
    "                                                                    order=[0, 1],\n",
    "                                                                    cval=(0, 255),\n",
    "                                                                    mode=ia.ALL))\n",
    "                                     ], random_order=True)\n",
    "                batch_features = seq.augment_images(batch_features)\n",
    "                \n",
    "                batch_mean = np.mean(batch_features)\n",
    "                batch_std = np.std(batch_features)\n",
    "                batch_features = batch_features.astype(np.float32)\n",
    "                for ii in range(batch_features.shape[0]):\n",
    "                    \n",
    "                    batch_features[ii, :, :, :] = (batch_features[ii, :, :, :] - batch_mean) / batch_std                     \n",
    "                train_neural_network(sess, optimizer, kp1, kp2, kp3, kp4, batch_features, batch_labels, True)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.9019705414012739\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9cb89d550>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "    test_features = test_features.astype(np.float32)\n",
    "    batch_mean = np.mean(test_features)\n",
    "    batch_std = np.std(test_features)\n",
    "    for ii in range(test_features.shape[0]):\n",
    "        test_features[ii,:,:,:] = (test_features[ii,:,:,:] - batch_mean) / batch_std\n",
    "\n",
    "    \n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob_1 = loaded_graph.get_tensor_by_name('keep_prob_1:0')\n",
    "        loaded_keep_prob_2 = loaded_graph.get_tensor_by_name('keep_prob_2:0')\n",
    "        loaded_keep_prob_3 = loaded_graph.get_tensor_by_name('keep_prob_3:0')\n",
    "        loaded_keep_prob_4 = loaded_graph.get_tensor_by_name('keep_prob_4:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        loaded_train_flag = loaded_graph.get_tensor_by_name('train_flag:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            \n",
    "            \n",
    "            test_batch_acc_total += sess.run(loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, \n",
    "                           loaded_keep_prob_1: 1.0, loaded_keep_prob_2: 1.0,\n",
    "                           loaded_keep_prob_3: 1.0, loaded_keep_prob_4: 1.0,\n",
    "                           loaded_train_flag:False})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        \n",
    "        random_test_predictions = sess.run(tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, \n",
    "                       loaded_keep_prob_1: 1.0, loaded_keep_prob_2: 1.0,\n",
    "                        loaded_keep_prob_3: 1.0, loaded_keep_prob_4: 1.0,\n",
    "                       loaded_train_flag:False})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_1_2]",
   "language": "python",
   "name": "conda-env-tf_1_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
